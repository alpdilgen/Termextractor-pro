# TermExtractor-Pro ğŸ”

**AI-powered terminology extraction with bilingual lookup and morphological variant discovery**

Extract key terms and concepts from documents using Claude AI, automatically find existing translations in bilingual files, and discover morphological variants of single-word terms.

---

## âœ¨ Features

### ğŸ¤– AI-Powered Extraction
- Uses Claude 3.5 (Haiku for speed, Sonnet for complex analysis)
- Cost-optimized model selection
- Intelligent domain classification
- Multi-level domain hierarchy support

### ğŸŒ Bilingual Lookup
- Exact matching with existing translations
- Fuzzy matching with configurable threshold
- Supports XLIFF, SDLXLIFF, MQXLIFF formats
- Automatic language detection
- Reduces API calls by reusing translations

### ğŸ”¬ Derivative Discovery
- Finds morphological variants (prefix/suffix additions)
- Configurable search patterns (prefix, suffix, any)
- Perfect for building comprehensive terminology databases
- Supports Unicode and special characters

### ğŸ“Š Multiple Export Formats
- **XLSX**: Spreadsheet with sheets for terms, derivatives, and statistics
- **CSV**: Standard comma-separated values
- **JSON**: Structured data for APIs and automation
- **TBX**: Terminology Exchange format for CAT tools

### ğŸ“ˆ Rich Statistics
- Relevance scoring (0-100)
- Confidence assessment
- Frequency counting
- Domain hierarchy tracking
- Bilingual lookup metrics
- Derivative discovery statistics

### ğŸ¯ Advanced Configuration
- Customizable relevance thresholds
- Domain-specific extraction hints
- Multi-language support (20+)
- Batch processing ready
- Comprehensive logging

---

## ğŸš€ Quick Start

### 1. Installation

```bash
git clone https://github.com/yourusername/TermExtractor-Pro.git
cd TermExtractor-Pro
pip install -r requirements.txt
```

### 2. Get API Key

1. Visit https://console.anthropic.com
2. Create a new API key
3. Copy the key

### 3. Run Application

```bash
streamlit run streamlit_app.py
```

### 4. Configure and Extract

1. Paste your API key in the sidebar
2. Go to **Extract** tab
3. Upload a document (PDF, DOCX, TXT, etc.)
4. Configure language pair and optional domain
5. Enable bilingual lookup and/or derivative discovery (optional)
6. Click **Extract Terms**
7. Review results in **Results** tab
8. Export in your preferred format

---

## ğŸ“– Usage Guide

### Basic Extraction

```python
from src.extraction import TermExtractor

extractor = TermExtractor(api_key="your-api-key")

result = extractor.extract(
    file_path="document.pdf",
    source_lang="en",
    target_lang="de",
    domain_path="Medical/Healthcare"
)

# Export results
xlsx_data = extractor.export_result(result, "xlsx")
```

### With Bilingual Lookup

```python
result = extractor.extract(
    file_path="document.pdf",
    source_lang="en",
    target_lang="de",
    enable_bilingual_lookup=True,
    bilingual_file_path="existing_translations.xliff",
    fuzzy_threshold=70
)
```

### With Derivative Discovery

```python
result = extractor.extract(
    file_path="document.pdf",
    source_lang="en",
    enable_derivative_discovery=True,
    derivative_modes=["prefix", "suffix"]
)
```

### Accessing Results

```python
# Get high-relevance terms
high_relevance = result.get_high_relevance_terms(threshold=80)

# Access statistics
print(result.statistics)
print(result.lookup_statistics)
print(result.derivative_statistics)

# Iterate through terms
for term in result.terms:
    print(f"{term.term} â†’ {term.translation}")
    print(f"  Domain: {term.domain}")
    print(f"  Relevance: {term.relevance_score}")
    print(f"  Derivatives: {term.discovered_derivatives}")
```

---

## ğŸ—‚ï¸ Project Structure

```
TermExtractor-Pro/
â”œâ”€â”€ streamlit_app.py              # Main Streamlit application
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ extraction.py             # Term extraction page
â”‚   â”œâ”€â”€ results.py                # Results display page
â”‚   â””â”€â”€ settings.py               # Settings and configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ config.py             # Configuration manager
â”‚   â”‚   â””â”€â”€ config.yaml           # Configuration settings
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ data_models.py        # Data classes (Term, ExtractionResult)
â”‚   â”œâ”€â”€ extraction/
â”‚   â”‚   â”œâ”€â”€ term_extractor.py     # Main extraction orchestrator
â”‚   â”‚   â”œâ”€â”€ bilingual_file_handler.py
â”‚   â”‚   â”œâ”€â”€ translation_lookup.py
â”‚   â”‚   â””â”€â”€ derivative_discovery.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ anthropic_client.py   # Anthropic API client
â”‚   â”‚   â””â”€â”€ api_manager.py        # API orchestration & caching
â”‚   â”œâ”€â”€ io/
â”‚   â”‚   â”œâ”€â”€ file_parser.py        # Multi-format file parsing
â”‚   â”‚   â””â”€â”€ format_exporter.py    # Multi-format export
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ constants.py          # Application constants
â”‚       â””â”€â”€ helpers.py            # Utility functions
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”§ Configuration

Edit `src/config/config.yaml` to customize:

```yaml
extraction:
  default_relevance_threshold: 70.0    # Default term quality threshold
  chunk_size: 2000                     # Characters per processing chunk

model_selection:
  extraction_model: "claude-3-5-haiku-20241022"  # Fast, cheap
  domain_classification_model: "claude-3-5-haiku-20241022"
  fuzzy_refinement_model: "claude-3-5-sonnet-20241022"  # Better reasoning

translation_lookup:
  enabled: false
  fuzzy_threshold: 70.0                # Minimum similarity for fuzzy match
  supported_formats: [xliff, sdlxliff, mqxliff]

derivative_discovery:
  enabled: false
  modes: [prefix, suffix]              # Search patterns
  max_variants_per_term: 20
```

---

## ğŸ¯ Supported Languages

English, German, French, Spanish, Italian, Portuguese, Bulgarian, Romanian, Turkish, Russian, Japanese, Chinese, Arabic, Hindi, and more (30+ languages).

---

## ğŸ“Š Export Examples

### XLSX Export Structure
```
Sheet 1: Terms
- Full term details with all fields
- Color-coded by relevance (green: 80+, yellow: 60-79, pink: <60)

Sheet 2: Derivatives (if found)
- Base term | Derivative Count | Variants

Sheet 3: Statistics
- Extraction statistics
- Bilingual lookup metrics
- Derivative discovery stats
```

### JSON Export Structure
```json
{
  "metadata": {
    "source_language": "en",
    "target_language": "de",
    "domain_hierarchy": ["Medical", "Healthcare"]
  },
  "terms": [...],
  "statistics": {...},
  "lookup_statistics": {...},
  "derivative_statistics": {...}
}
```

---

## ğŸ’° Cost Optimization

TermExtractor-Pro is designed for cost efficiency:

- **Model Selection**: Uses Haiku (4x cheaper than Sonnet) for simple extraction, Sonnet only for complex analysis
- **Caching**: Results cached to avoid redundant API calls
- **Bilingual Lookup**: Reduces API calls by reusing existing translations
- **Token Optimization**: Efficient prompts that minimize token usage

**Estimated Costs:**
- Small document (1000 words): ~$0.01
- Medium document (5000 words): ~$0.05
- Large document (20,000 words): ~$0.20

---

## ğŸ”’ Security & Privacy

- API keys are never logged or stored
- All data is sent securely to Anthropic servers
- No data retention on TermExtractor-Pro servers
- GDPR and privacy-aware design
- Encryption for sensitive data in transit

---

## ğŸš€ Deployment

### Deploy to Streamlit Cloud

1. Push to GitHub
2. Visit https://share.streamlit.io
3. Connect your repository
4. Set `ANTHROPIC_API_KEY` secret in Streamlit dashboard
5. Deploy!

### Deploy to Other Platforms

Works with any Python hosting that supports Streamlit:
- Heroku
- Railway
- PythonAnywhere
- AWS / Azure / GCP
- Docker containers

---

## ğŸ“ Supported Input Formats

- **Documents**: PDF, DOCX, TXT, HTML
- **Bilingual Files**: XLIFF, SDLXLIFF, MQXLIFF, XML
- **Max Size**: 100 MB
- **Languages**: Auto-detect or manual selection

---

## ğŸ¤ Advanced Features

### For Professional Users

- **Termbase Integration**: Import and match against existing termbases
- **Quality Metrics**: Precision, recall, F1 scores
- **Batch Processing**: Process multiple documents efficiently
- **API Mode**: Use as FastAPI backend (coming soon)
- **Custom Prompts**: Fine-tune extraction behavior

### For Developers

- Modular architecture for easy customization
- Cost-optimized API client
- Rate limiting and caching built-in
- Comprehensive logging
- RESTful API ready

---

## ğŸ“Š Statistics & Metrics

Each extraction provides:

- **Relevance Score** (0-100): How important is the term to the text?
- **Confidence Score** (0-100): How confident is the AI in this extraction?
- **Frequency**: How many times does the term appear?
- **Domain Hierarchy**: Multi-level domain classification
- **Translation Source**: Where did the translation come from? (API, Exact, Fuzzy)
- **Fuzzy Match Score**: Similarity % for fuzzy matches
- **Discovered Variants**: Morphological forms of the term

---

## ğŸ› Troubleshooting

### "API Key not found"
- Ensure you've set `ANTHROPIC_API_KEY` environment variable
- Or enter the key in the Streamlit sidebar

### "File parsing error"
- Ensure file is not corrupted
- Try exporting to another format and re-uploading
- Check file size (max 100 MB)

### "No terms extracted"
- Increase relevance threshold to show more terms
- Check if domain hint is too restrictive
- Verify source language is correct

### "Fuzzy matching not working"
- Increase fuzzy threshold (currently at what value?)
- Enable bilingual file upload
- Check file format is XLIFF/SDLXLIFF/MQXLIFF

---

## ğŸ“š Documentation

- **User Guide**: See Settings tab in app
- **API Documentation**: Full docstrings in source code
- **Configuration**: Edit `src/config/config.yaml`
- **Examples**: See usage section above

---

## ğŸ“ Learning Resources

- [Anthropic Documentation](https://docs.anthropic.com)
- [Claude Models](https://www.anthropic.com/product)
- [Terminology Extraction Best Practices](https://en.wikipedia.org/wiki/Terminology_extraction)
- [XLIFF Standard](https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=xliff)

---

## ğŸ“„ License

This project is open source and available under the MIT License.

---

## ğŸ™‹ Support

- **Issues**: Report bugs on GitHub
- **Discussions**: Ask questions in GitHub discussions
- **Email**: support@example.com

---

## ğŸ‰ Acknowledgments

Built with:
- [Anthropic Claude API](https://www.anthropic.com)
- [Streamlit](https://streamlit.io)
- [Python](https://www.python.org)

---

**Happy Terminology Extraction! ğŸš€**

Made with â¤ï¸ for translators, terminologists, and localization professionals worldwide.
